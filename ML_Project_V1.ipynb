{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Project-V1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnberg1/MusicGenrePredictor/blob/master/ML_Project_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7kW6mshtAqz",
        "colab_type": "code",
        "outputId": "235ebf5d-d6b3-4df7-9684-ef6d197f79af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Mounting Google Drive, only needed once\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBytvuIrhjBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function for removing unnecessary characters\n",
        "def getgenres(temp):\n",
        "    temp = temp.replace(\"[\",\"\")\n",
        "    temp = temp.replace(\",\",\"\")\n",
        "    temp = temp.replace(\"]\",\"\")\n",
        "    lis = list(map(int,temp.split(\" \")))\n",
        "    arr = np.asarray(lis)\n",
        "    return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCM0Qtm4RHE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Uploading the arrays into local variables, only needed once\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/ML PROJECT/genresV1.csv')\n",
        "genres = df.to_numpy()   #to convert pandas dataframe to 2d numpy array\n",
        "df = pd.read_csv('/content/drive/My Drive/ML PROJECT/features.csv',header=None )\n",
        "features = df.to_numpy()   #to convert pandas dataframe to 2d numpy array\n",
        "\n",
        "#fixing issue with geners data format\n",
        "temp = []\n",
        "\n",
        "for i in range(np.size(genres,0)):\n",
        "  temp.append(genres[i][0])\n",
        "temp = np.asarray(temp)\n",
        "\n",
        "genres2 = []\n",
        "features2= []\n",
        "\n",
        "c = 0\n",
        "for i in temp:\n",
        "  if( i != '[]' and i != 'nan' and i != 'Jarrod Fowler' and i != ' John Butcher' and (i[0:5] != \"['zal\") ):\n",
        "    genres2.append(getgenres(i))\n",
        "    features2.append( features[c])\n",
        "  c +=1\n",
        "\n",
        "genres2 = np.asarray(genres2)\n",
        "features2 = np.asarray(features2)\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/ML PROJECT/genres.csv')\n",
        "genrelist = df.to_numpy()   #to convert pandas dataframe to 2d numpy array\n",
        "temp1 = np.transpose(genrelist)[0]#all genres\n",
        "temp2 = np.transpose(genrelist)[4] #parent genres, if 0 -> it is already a parent\n",
        "genrelist = np.transpose(np.vstack((temp1,temp2))) #we only need these 2 at this point\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efkPE7HVkJwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genresSingle = []\n",
        "featuresSingle = [] \n",
        "for (i,j) in zip(genres2,features2):\n",
        "  for k in i:\n",
        "    genresSingle.append(k)\n",
        "    featuresSingle.append(j)\n",
        "genresSingle = np.asarray(genresSingle)\n",
        "featuresSingle = np.asarray(featuresSingle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF-9CfWHkOUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Maps each genre to their top genres and treats them as separate songs\n",
        "genresSingleTop = []\n",
        "featuresSingleTop = [] \n",
        "for (i,j) in zip(genres2,features2):\n",
        "  for k in i:\n",
        "    t = temp2[np.where(temp1 == k)]\n",
        "    genresSingleTop.append(t[0])\n",
        "    featuresSingleTop.append(j)\n",
        "genresSingleTop = np.asarray(genresSingleTop)\n",
        "featuresSingleTop = np.asarray(featuresSingleTop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6o8v1eYyI3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genresSingleTop2 = []\n",
        "featuresSingleTop2 = [] \n",
        "for (i,j) in zip(genres2,features2):\n",
        "  xyt = []\n",
        "  for k in i:\n",
        "    t = temp2[np.where(temp1 == k)]\n",
        "    if t[0] not in xyt:\n",
        "      xyt.append(t[0])\n",
        "\n",
        "      genresSingleTop2.append(t[0])\n",
        "      featuresSingleTop2.append(j)\n",
        "genresSingleTop2 = np.asarray(genresSingleTop2)\n",
        "featuresSingleTop2 = np.asarray(featuresSingleTop2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaipngK-0rGS",
        "colab_type": "code",
        "outputId": "4f7a0b54-fe82-49ce-91c4-1b5851f02de5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "xy = genres2[-3]\n",
        "xyt = []\n",
        "print(xy)\n",
        "for j in xy:\n",
        "  xyz = np.where(temp1 == j)\n",
        "  print(xyz)\n",
        "  xyt.append(temp2[xyz[0][0]])\n",
        "print (type(xyt))\n",
        "print(max(set(xyt), key = xyt.count))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[12 32 66]\n",
            "(array([11]),)\n",
            "(array([27]),)\n",
            "(array([44]),)\n",
            "<class 'list'>\n",
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quifmQlM5hiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fatbardh\n",
        "from operator import itemgetter\n",
        "\n",
        "countGenres = np.zeros(1236)\n",
        "for i in range(81553):\n",
        "  for j in genres2[i]:\n",
        "    countGenres[j] += 1\n",
        "\n",
        "graphData = []\n",
        "for i in range(1236):\n",
        "  if(countGenres[i] != 0.0):\n",
        "    graphData.append( [i , int(countGenres[i])])\n",
        "\n",
        "a = sorted(graphData, key=lambda x: x[1], reverse=True)\n",
        "print(len( graphData))\n",
        "for i in range(15):\n",
        "  print(a[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Oxvtu3B2kLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(genres2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8bZzndJ0c6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genresSingleTop = []\n",
        "featuresSingleTop = [] \n",
        "for (i,j) in zip(genres2,features2):\n",
        "    t = temp2[np.where(temp1 == i)]\n",
        "  for k in i:\n",
        "    genresSingleTop.append(t[0])\n",
        "    featuresSingleTop.append(j)\n",
        "genresSingleTop = np.asarray(genresSingleTop)\n",
        "featuresSingleTop = np.asarray(featuresSingleTop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ5GJe_wuVQX",
        "colab_type": "code",
        "outputId": "ca8f1d6b-12be-4ca1-9ff5-a1abeaa8e6ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#Sanity check for sizes. The last to should be (191292,518) and (191292,)\n",
        "print(\"Geners.shape\")\n",
        "print( genres2.shape)\n",
        "print(\"Features List shape.\")\n",
        "print(features2.shape)\n",
        "print(\"GenreList Shape\")\n",
        "print( genrelist.shape)\n",
        "\n",
        "\n",
        "print(\"Feature Matrix Shape:\")\n",
        "print(featuresSingleTop.shape)\n",
        "print(\"Genres Vector Shape:\")\n",
        "print(genresSingleTop.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Geners.shape\n",
            "(81553,)\n",
            "Features List shape.\n",
            "(81553, 518)\n",
            "GenreList Shape\n",
            "(163, 2)\n",
            "Feature Matrix Shape:\n",
            "(191292, 518)\n",
            "Genres Vector Shape:\n",
            "(191292,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afw2_KVfeYs9",
        "colab_type": "code",
        "outputId": "112beeff-40bc-4855-f5f2-2e12905207ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "##### ANN PART #####\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
        "import keras\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# Get the features and genres after processing\n",
        "featuresTrial = featuresSingleTop\n",
        "genresTrial = genresSingleTop\n",
        "\n",
        "# Encoding each label so they have values from 1 to 16\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(genresTrial)\n",
        "\n",
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(np.array(featuresTrial, dtype = float))\n",
        "\n",
        "# 0.8 training 0.2 test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Defining model parameters\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(16, activation='softmax'))\n",
        "\n",
        "# Running and fitting our model\n",
        "model.compile(optimizer='nadam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs=150,\n",
        "                    batch_size=1024)\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "153033/153033 [==============================] - 1s 9us/step - loss: 2.1387 - acc: 0.2493\n",
            "Epoch 2/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 2.0784 - acc: 0.2676\n",
            "Epoch 3/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 2.0510 - acc: 0.2819\n",
            "Epoch 4/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 2.0174 - acc: 0.3004\n",
            "Epoch 5/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.9762 - acc: 0.3202\n",
            "Epoch 6/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.9278 - acc: 0.3424\n",
            "Epoch 7/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.8694 - acc: 0.3657\n",
            "Epoch 8/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.8112 - acc: 0.3886\n",
            "Epoch 9/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.7540 - acc: 0.4103\n",
            "Epoch 10/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.6973 - acc: 0.4290\n",
            "Epoch 11/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.6405 - acc: 0.4476\n",
            "Epoch 12/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.5953 - acc: 0.4615\n",
            "Epoch 13/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.5421 - acc: 0.4778\n",
            "Epoch 14/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.4978 - acc: 0.4922\n",
            "Epoch 15/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.4616 - acc: 0.5000\n",
            "Epoch 16/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.4207 - acc: 0.5129\n",
            "Epoch 17/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.3895 - acc: 0.5202\n",
            "Epoch 18/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.3566 - acc: 0.5281\n",
            "Epoch 19/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.3249 - acc: 0.5376\n",
            "Epoch 20/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.2991 - acc: 0.5428\n",
            "Epoch 21/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.2749 - acc: 0.5485\n",
            "Epoch 22/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.2518 - acc: 0.5542\n",
            "Epoch 23/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.2323 - acc: 0.5581\n",
            "Epoch 24/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.2118 - acc: 0.5632\n",
            "Epoch 25/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.1889 - acc: 0.5687\n",
            "Epoch 26/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.1728 - acc: 0.5728\n",
            "Epoch 27/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.1549 - acc: 0.5771\n",
            "Epoch 28/150\n",
            "153033/153033 [==============================] - 1s 8us/step - loss: 1.1404 - acc: 0.5801\n",
            "Epoch 29/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.1256 - acc: 0.5835\n",
            "Epoch 30/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.1094 - acc: 0.5872\n",
            "Epoch 31/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.1007 - acc: 0.5884\n",
            "Epoch 32/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.0868 - acc: 0.5907\n",
            "Epoch 33/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.0744 - acc: 0.5938\n",
            "Epoch 34/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.0649 - acc: 0.5971\n",
            "Epoch 35/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.0514 - acc: 0.6003\n",
            "Epoch 36/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.0431 - acc: 0.6009\n",
            "Epoch 37/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.0332 - acc: 0.6035\n",
            "Epoch 38/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.0237 - acc: 0.6053\n",
            "Epoch 39/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.0182 - acc: 0.6065\n",
            "Epoch 40/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 1.0069 - acc: 0.6079\n",
            "Epoch 41/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9997 - acc: 0.6090\n",
            "Epoch 42/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9911 - acc: 0.6116\n",
            "Epoch 43/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9801 - acc: 0.6144\n",
            "Epoch 44/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9753 - acc: 0.6141\n",
            "Epoch 45/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9697 - acc: 0.6155\n",
            "Epoch 46/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9632 - acc: 0.6181\n",
            "Epoch 47/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9580 - acc: 0.6184\n",
            "Epoch 48/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9512 - acc: 0.6194\n",
            "Epoch 49/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9467 - acc: 0.6211\n",
            "Epoch 50/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9368 - acc: 0.6241\n",
            "Epoch 51/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9372 - acc: 0.6221\n",
            "Epoch 52/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9306 - acc: 0.6238\n",
            "Epoch 53/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9254 - acc: 0.6243\n",
            "Epoch 54/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9208 - acc: 0.6250\n",
            "Epoch 55/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9144 - acc: 0.6279\n",
            "Epoch 56/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9103 - acc: 0.6276\n",
            "Epoch 57/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9074 - acc: 0.6283\n",
            "Epoch 58/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.9016 - acc: 0.6302\n",
            "Epoch 59/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8946 - acc: 0.6314\n",
            "Epoch 60/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8957 - acc: 0.6312\n",
            "Epoch 61/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8893 - acc: 0.6321\n",
            "Epoch 62/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8853 - acc: 0.6335\n",
            "Epoch 63/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8822 - acc: 0.6335\n",
            "Epoch 64/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8793 - acc: 0.6338\n",
            "Epoch 65/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8746 - acc: 0.6354\n",
            "Epoch 66/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8729 - acc: 0.6357\n",
            "Epoch 67/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8692 - acc: 0.6348\n",
            "Epoch 68/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8692 - acc: 0.6369\n",
            "Epoch 69/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8595 - acc: 0.6379\n",
            "Epoch 70/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8593 - acc: 0.6386\n",
            "Epoch 71/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8560 - acc: 0.6383\n",
            "Epoch 72/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8550 - acc: 0.6389\n",
            "Epoch 73/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8523 - acc: 0.6389\n",
            "Epoch 74/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8486 - acc: 0.6394\n",
            "Epoch 75/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8482 - acc: 0.6389\n",
            "Epoch 76/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8417 - acc: 0.6402\n",
            "Epoch 77/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8405 - acc: 0.6404\n",
            "Epoch 78/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8376 - acc: 0.6433\n",
            "Epoch 79/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8363 - acc: 0.6419\n",
            "Epoch 80/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8333 - acc: 0.6425\n",
            "Epoch 81/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8318 - acc: 0.6428\n",
            "Epoch 82/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8287 - acc: 0.6437\n",
            "Epoch 83/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8267 - acc: 0.6443\n",
            "Epoch 84/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8235 - acc: 0.6444\n",
            "Epoch 85/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8222 - acc: 0.6443\n",
            "Epoch 86/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8201 - acc: 0.6452\n",
            "Epoch 87/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8188 - acc: 0.6453\n",
            "Epoch 88/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8150 - acc: 0.6457\n",
            "Epoch 89/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8149 - acc: 0.6450\n",
            "Epoch 90/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8124 - acc: 0.6473\n",
            "Epoch 91/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8093 - acc: 0.6475\n",
            "Epoch 92/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8184 - acc: 0.6448\n",
            "Epoch 93/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8077 - acc: 0.6466\n",
            "Epoch 94/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8033 - acc: 0.6484\n",
            "Epoch 95/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7991 - acc: 0.6500\n",
            "Epoch 96/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.8012 - acc: 0.6478\n",
            "Epoch 97/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7982 - acc: 0.6499\n",
            "Epoch 98/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7976 - acc: 0.6493\n",
            "Epoch 99/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7973 - acc: 0.6495\n",
            "Epoch 100/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7953 - acc: 0.6498\n",
            "Epoch 101/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7915 - acc: 0.6507\n",
            "Epoch 102/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7899 - acc: 0.6520\n",
            "Epoch 103/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7881 - acc: 0.6519\n",
            "Epoch 104/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7868 - acc: 0.6514\n",
            "Epoch 105/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7855 - acc: 0.6512\n",
            "Epoch 106/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7837 - acc: 0.6519\n",
            "Epoch 107/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7831 - acc: 0.6525\n",
            "Epoch 108/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7826 - acc: 0.6521\n",
            "Epoch 109/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7787 - acc: 0.6524\n",
            "Epoch 110/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7798 - acc: 0.6538\n",
            "Epoch 111/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7777 - acc: 0.6529\n",
            "Epoch 112/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7767 - acc: 0.6522\n",
            "Epoch 113/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7738 - acc: 0.6544\n",
            "Epoch 114/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7745 - acc: 0.6530\n",
            "Epoch 115/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7718 - acc: 0.6529\n",
            "Epoch 116/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7709 - acc: 0.6545\n",
            "Epoch 117/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7696 - acc: 0.6550\n",
            "Epoch 118/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7659 - acc: 0.6558\n",
            "Epoch 119/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7662 - acc: 0.6551\n",
            "Epoch 120/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7665 - acc: 0.6556\n",
            "Epoch 121/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7621 - acc: 0.6570\n",
            "Epoch 122/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7631 - acc: 0.6562\n",
            "Epoch 123/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7618 - acc: 0.6567\n",
            "Epoch 124/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7599 - acc: 0.6563\n",
            "Epoch 125/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7616 - acc: 0.6560\n",
            "Epoch 126/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7584 - acc: 0.6561\n",
            "Epoch 127/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7580 - acc: 0.6572\n",
            "Epoch 128/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7546 - acc: 0.6573\n",
            "Epoch 129/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7537 - acc: 0.6575\n",
            "Epoch 130/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7521 - acc: 0.6587\n",
            "Epoch 131/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7512 - acc: 0.6577\n",
            "Epoch 132/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7531 - acc: 0.6579\n",
            "Epoch 133/150\n",
            "153033/153033 [==============================] - 1s 8us/step - loss: 0.7488 - acc: 0.6582\n",
            "Epoch 134/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7503 - acc: 0.6575\n",
            "Epoch 135/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7469 - acc: 0.6600\n",
            "Epoch 136/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7453 - acc: 0.6601\n",
            "Epoch 137/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7455 - acc: 0.6576\n",
            "Epoch 138/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7435 - acc: 0.6586\n",
            "Epoch 139/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7423 - acc: 0.6598\n",
            "Epoch 140/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7414 - acc: 0.6595\n",
            "Epoch 141/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7423 - acc: 0.6587\n",
            "Epoch 142/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7387 - acc: 0.6590\n",
            "Epoch 143/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7388 - acc: 0.6587\n",
            "Epoch 144/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7398 - acc: 0.6586\n",
            "Epoch 145/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7361 - acc: 0.6617\n",
            "Epoch 146/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7378 - acc: 0.6599\n",
            "Epoch 147/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7380 - acc: 0.6605\n",
            "Epoch 148/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7348 - acc: 0.6607\n",
            "Epoch 149/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7330 - acc: 0.6595\n",
            "Epoch 150/150\n",
            "153033/153033 [==============================] - 1s 7us/step - loss: 0.7330 - acc: 0.6618\n",
            "top1 acc 0.6974835492998243\n",
            "top3 acc 0.980755784700032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFRz5_qnO-HY",
        "colab_type": "code",
        "outputId": "19819bf3-0b40-4337-92a3-522d3ba41eb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Compute top1 and top3 accuracies\n",
        "top1 = 0.0\n",
        "top3 = 0.0    \n",
        "class_probs = model.predict(X_test)\n",
        "for i, l in enumerate(y_test):\n",
        "    class_prob = class_probs[i]\n",
        "    top_values = (-class_prob).argsort()[:3]\n",
        "    if top_values[0] == l:\n",
        "        top1 += 1.0\n",
        "    if np.isin(np.array([l]), top_values):\n",
        "        top3 += 1.0\n",
        "\n",
        "print(\"top1 acc\", top1/len(y_test))\n",
        "print(\"top3 acc\", top3/len(y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top1 acc 0.37120677487649967\n",
            "top3 acc 0.6535717086175802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49ue_SC6-3lp",
        "colab_type": "code",
        "outputId": "fc0bfcc9-3625-4ab7-f94c-c24ea090966e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "# Plotting the loss and accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['acc'])\n",
        "plt.xlabel('epoch #')\n",
        "plt.ylabel('Training Accuracy')\n",
        "plt.title('Training Accuracy vs epoch #')\n",
        "plt.figure()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.xlabel('epoch #')\n",
        "plt.ylabel('Sparse Categorical Cross-Entropy')\n",
        "plt.title('Training Loss vs epoch #')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Training Loss vs epoch #')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzrkk9/8BwWQBCEVlUlYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEZ0rKyWNvEt",
        "colab_type": "code",
        "outputId": "04e24493-613a-41cd-c78a-fdf852623c69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!pip install torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/95/90e8c4c31cfc67248bf944ba42029295b77159982f532c5689bcfe4e9108/torch-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (734.6MB)\n",
            "\u001b[K     |████████████████████████████████| 734.6MB 24kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3MTK-hhWmpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### CNN PART #####\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Get the features and genres after processing\n",
        "featuresTrial = featuresSingleTop\n",
        "genresTrial = genresSingleTop\n",
        "\n",
        "# Encoding each label so they have values from 1 to 16\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(genresTrial)\n",
        "\n",
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(np.array(featuresTrial, dtype = float))\n",
        "\n",
        "# 0.6 train 0.2 validation 0.2 test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo45Hi3wL4Zf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding dimension for compatibility\n",
        "X_train = X_train[: ,None, :]\n",
        "X_test = X_test[:, None,:]\n",
        "X_val = X_val[:, None,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fcPV4TxPBCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhzVNIOSBXS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting to tensors\n",
        "X_train = torch.from_numpy(X_train)\n",
        "X_test = torch.from_numpy(X_test)\n",
        "y_train = torch.from_numpy(y_train)\n",
        "y_test = torch.from_numpy(y_test)\n",
        "X_val = torch.from_numpy(X_val)\n",
        "y_val = torch.from_numpy(y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCDRTZk4-y44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Overwriting the Dataset function\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "class Dataset(Dataset):\n",
        "  #Characterizes a dataset for PyTorch\n",
        "  def __init__(self, y_train, X_train):\n",
        "        #Initialization'\n",
        "        self.X = X_train\n",
        "        self.Y = y_train\n",
        "\n",
        "  def __len__(self):\n",
        "        #Denotes the total number of samples'\n",
        "        return len(self.X)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        #Generates one sample of data'\n",
        "        return (self.X[index],self.Y[index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywigEWtuCFWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dataset and data loaders\n",
        "train_data = Dataset(y_train, X_train)\n",
        "test_data = Dataset(y_test,X_test)\n",
        "val_data = Dataset(y_val,X_val)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_data,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyDC43e8J0C6",
        "colab_type": "code",
        "outputId": "743d6cb2-bae0-4f42-9a9e-195dd5f424e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Sanity check\n",
        "x,y = train_data[0]\n",
        "print(x.shape,y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 518]) tensor(10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDB6hcvVhQmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNN Model\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv1d(1, 20, kernel_size=5)\n",
        "        self.conv2 = nn.Conv1d(10, 50, kernel_size=5)\n",
        "        self.conv3 = nn.Conv1d(25,100, kernel_size=5)\n",
        "        self.mp = nn.MaxPool2d(2)\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(3050, 256)\n",
        "        self.fc2 = nn.Linear(256,16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        in_size = x.size(0)\n",
        "        x = F.relu(self.mp(self.conv1(x)))\n",
        "        x = F.relu(self.mp(self.conv2(x)))\n",
        "        x = F.relu(self.mp(self.conv3(x)))\n",
        "        x = x.view(in_size, -1)  # flatten the tensor\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x) #numerical stability\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAONt7Urw3tZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net()\n",
        "# Selecting the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8qO-59TxL8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training function\n",
        "def train(epoch):\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        data = data.float()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * data.size(0) \n",
        "        if batch_idx % 200 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    losses.append(epoch_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPyVUiMCxQ1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test function\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    correct3 = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        data = data.float()\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "        # get the index of the max log-probability\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        pred3,indx = output.data.topk(3)\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "        for i in range(len(data)):\n",
        "            if (target.data[i].item() in indx[i]):\n",
        "              correct3 += 1\n",
        "        acc3 = correct3 * 100 / len(test_loader.dataset)\n",
        "    print(target.data[0].item())\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    print('\\nTest set: Top-3 Accuracy: {:.4f}\\n' .format(acc3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAHBPxNYLFcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Validation function\n",
        "def validate():\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in val_loader:\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        data = data.float()\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        val_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "        # get the index of the max log-probability\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "    val_acc.append(100. * correct / len(val_loader.dataset))\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_losses.append(val_loss)\n",
        "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(val_loss, correct, len(val_loader.dataset),\n",
        "        100. * correct / len(val_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X86p3gvuxUDu",
        "colab_type": "code",
        "outputId": "78e9d707-41d6-469e-a712-2124ac2c4839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Running and fitting the model\n",
        "losses = []\n",
        "val_losses = []\n",
        "val_acc = []\n",
        "for epoch in range(1, 6):\n",
        "    train(epoch)\n",
        "    validate()\n",
        "test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/114774 (0%)]\tLoss: 2.772017\n",
            "Train Epoch: 1 [12800/114774 (11%)]\tLoss: 2.126332\n",
            "Train Epoch: 1 [25600/114774 (22%)]\tLoss: 1.840971\n",
            "Train Epoch: 1 [38400/114774 (33%)]\tLoss: 2.127621\n",
            "Train Epoch: 1 [51200/114774 (45%)]\tLoss: 1.985627\n",
            "Train Epoch: 1 [64000/114774 (56%)]\tLoss: 2.098079\n",
            "Train Epoch: 1 [76800/114774 (67%)]\tLoss: 2.123934\n",
            "Train Epoch: 1 [89600/114774 (78%)]\tLoss: 2.101003\n",
            "Train Epoch: 1 [102400/114774 (89%)]\tLoss: 2.007358\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation set: Average loss: 2.1004, Accuracy: 9906/38259 (26%)\n",
            "\n",
            "Train Epoch: 2 [0/114774 (0%)]\tLoss: 1.992044\n",
            "Train Epoch: 2 [12800/114774 (11%)]\tLoss: 1.964974\n",
            "Train Epoch: 2 [25600/114774 (22%)]\tLoss: 2.075906\n",
            "Train Epoch: 2 [38400/114774 (33%)]\tLoss: 2.180037\n",
            "Train Epoch: 2 [51200/114774 (45%)]\tLoss: 2.057306\n",
            "Train Epoch: 2 [64000/114774 (56%)]\tLoss: 2.008744\n",
            "Train Epoch: 2 [76800/114774 (67%)]\tLoss: 2.172167\n",
            "Train Epoch: 2 [89600/114774 (78%)]\tLoss: 2.020135\n",
            "Train Epoch: 2 [102400/114774 (89%)]\tLoss: 1.854748\n",
            "\n",
            "Validation set: Average loss: 2.0936, Accuracy: 9890/38259 (26%)\n",
            "\n",
            "Train Epoch: 3 [0/114774 (0%)]\tLoss: 2.151354\n",
            "Train Epoch: 3 [12800/114774 (11%)]\tLoss: 1.869942\n",
            "Train Epoch: 3 [25600/114774 (22%)]\tLoss: 2.123109\n",
            "Train Epoch: 3 [38400/114774 (33%)]\tLoss: 2.158857\n",
            "Train Epoch: 3 [51200/114774 (45%)]\tLoss: 2.136269\n",
            "Train Epoch: 3 [64000/114774 (56%)]\tLoss: 2.134433\n",
            "Train Epoch: 3 [76800/114774 (67%)]\tLoss: 2.170188\n",
            "Train Epoch: 3 [89600/114774 (78%)]\tLoss: 2.130090\n",
            "Train Epoch: 3 [102400/114774 (89%)]\tLoss: 2.347517\n",
            "\n",
            "Validation set: Average loss: 2.0939, Accuracy: 9964/38259 (26%)\n",
            "\n",
            "Train Epoch: 4 [0/114774 (0%)]\tLoss: 2.033238\n",
            "Train Epoch: 4 [12800/114774 (11%)]\tLoss: 2.066667\n",
            "Train Epoch: 4 [25600/114774 (22%)]\tLoss: 1.992718\n",
            "Train Epoch: 4 [38400/114774 (33%)]\tLoss: 2.113282\n",
            "Train Epoch: 4 [51200/114774 (45%)]\tLoss: 2.215140\n",
            "Train Epoch: 4 [64000/114774 (56%)]\tLoss: 2.128481\n",
            "Train Epoch: 4 [76800/114774 (67%)]\tLoss: 2.101866\n",
            "Train Epoch: 4 [89600/114774 (78%)]\tLoss: 2.047361\n",
            "Train Epoch: 4 [102400/114774 (89%)]\tLoss: 2.143512\n",
            "\n",
            "Validation set: Average loss: 2.0905, Accuracy: 10020/38259 (26%)\n",
            "\n",
            "Train Epoch: 5 [0/114774 (0%)]\tLoss: 2.050610\n",
            "Train Epoch: 5 [12800/114774 (11%)]\tLoss: 1.935313\n",
            "Train Epoch: 5 [25600/114774 (22%)]\tLoss: 2.088975\n",
            "Train Epoch: 5 [38400/114774 (33%)]\tLoss: 1.890401\n",
            "Train Epoch: 5 [51200/114774 (45%)]\tLoss: 2.136339\n",
            "Train Epoch: 5 [64000/114774 (56%)]\tLoss: 2.071386\n",
            "Train Epoch: 5 [76800/114774 (67%)]\tLoss: 2.123709\n",
            "Train Epoch: 5 [89600/114774 (78%)]\tLoss: 2.119447\n",
            "Train Epoch: 5 [102400/114774 (89%)]\tLoss: 2.162268\n",
            "\n",
            "Validation set: Average loss: 2.0974, Accuracy: 10194/38259 (27%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "\n",
            "Test set: Average loss: 2.0939, Accuracy: 10007/38259 (26%)\n",
            "\n",
            "\n",
            "Test set: Top-3 Accuracy: 66.4994\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR-GnDBg1Azk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting losses\n",
        "plt.figure()\n",
        "plt.plot(range(1,6),np.array(losses), 'r')\n",
        "plt.xlabel('epoch #')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Training Loss vs epoch #')\n",
        "plt.figure()\n",
        "plt.plot(range(1,6),np.array(val_losses),'g')\n",
        "plt.xlabel('epoch #')\n",
        "plt.ylabel('Validation Loss')\n",
        "plt.title('Validation Loss vs epoch #')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}